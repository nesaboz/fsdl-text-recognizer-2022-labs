{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlH0lCOttCs5"
      },
      "source": [
        "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUPRHaeetRnT"
      },
      "source": [
        "# Lab 02a: PyTorch Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bry3Hr-PcgDs"
      },
      "source": [
        "### What You Will Learn\n",
        "\n",
        "- The core components of a PyTorch Lightning training loop: `LightningModule`s and `Trainer`s.\n",
        "- Useful quality-of-life improvements offered by PyTorch Lightning: `LightningDataModule`s, `Callback`s, and `Metric`s\n",
        "- How we use these features in the FSDL codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs0LXXlCU6Ix"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkQiK7lkgeXm"
      },
      "source": [
        "If you're running this notebook on Google Colab,\n",
        "the cell below will run full environment setup.\n",
        "\n",
        "It should take about three minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sVx7C7H0PIZC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=.:\n",
            ".:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/fsdl-text-recognizer-2022-labs/lab02\n",
            "\u001b[0m\u001b[01;34mlightning_logs\u001b[0m/  \u001b[01;34mnotebooks\u001b[0m/  \u001b[01;34mtext_recognizer\u001b[0m/  \u001b[01;34mtraining\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "lab_idx = 2\n",
        "\n",
        "if \"bootstrap\" not in locals() or bootstrap.run:\n",
        "    # path management for Python\n",
        "    pythonpath, = !echo $PYTHONPATH\n",
        "    if \".\" not in pythonpath.split(\":\"):\n",
        "        pythonpath = \".:\" + pythonpath\n",
        "        %env PYTHONPATH={pythonpath}\n",
        "        !echo $PYTHONPATH\n",
        "\n",
        "    # get both Colab and local notebooks into the same state\n",
        "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
        "    import bootstrap\n",
        "\n",
        "    # change into the lab directory\n",
        "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
        "\n",
        "    # allow \"hot-reloading\" of modules\n",
        "    %load_ext autoreload\n",
        "    %autoreload 2\n",
        "    # needed for inline plots in some contexts\n",
        "    %matplotlib inline\n",
        "\n",
        "    bootstrap.run = False  # change to True re-run setup\n",
        "    \n",
        "!pwd\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZN4bGgsgWc_"
      },
      "source": [
        "# Why Lightning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP8iJW_bg7IC"
      },
      "source": [
        "PyTorch is a powerful library for executing differentiable\n",
        "tensor operations with hardware acceleration\n",
        "and it includes many neural network primitives,\n",
        "but it has no concept of \"training\".\n",
        "At a high level, an `nn.Module` is a stateful function with gradients\n",
        "and a `torch.optim.Optimizer` can update that state using gradients,\n",
        "but there's no pre-built tools in PyTorch to iteratively generate those gradients from data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7gIA-Efy91E"
      },
      "source": [
        "So the first thing many folks do in PyTorch is write that code --\n",
        "a \"training loop\" to iterate over their `DataLoader`,\n",
        "which in pseudocode might look something like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3ewkWrwzDA8"
      },
      "source": [
        "```python\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = some_loss_function(targets, outputs)\n",
        "    \n",
        "    optimizer.zero_gradients()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYUtiJWize82"
      },
      "source": [
        "This is a solid start, but other needs immediately arise.\n",
        "You'll want to run your model on validation and test data,\n",
        "which need their own `DataLoader`s.\n",
        "Once finished, you'll want to save your model --\n",
        "and for long-running jobs, you probably want\n",
        "to save checkpoints of the training process\n",
        "so that it can be resumed in case of a crash.\n",
        "For state-of-the-art model performance in many domains,\n",
        "you'll want to distribute your training across multiple nodes/machines\n",
        "and across multiple GPUs within those nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0untumvjy5fm"
      },
      "source": [
        "That's just the tip of the iceberg, and you want\n",
        "all those features to work for lots of models and datasets,\n",
        "not just the one you're writing now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNPpi4OZjMbu"
      },
      "source": [
        "You don't want to write all of this yourself.\n",
        "\n",
        "So unless you are at a large organization that has a dedicated team\n",
        "for building that \"framework\" code,\n",
        "you'll want to use an existing library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnQuyVqUjJy8"
      },
      "source": [
        "PyTorch Lightning is a popular framework on top of PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7ecipNFTgZDt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://pytorch-lightning.readthedocs.io/en/1.6.3/'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "version = pl.__version__\n",
        "\n",
        "docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/\"  # version can also be latest, stable\n",
        "docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE82xoEikWkh"
      },
      "source": [
        "At its core, PyTorch Lightning provides\n",
        "\n",
        "1. the `pl.Trainer` class, which organizes and executes your training, validation, and test loops, and\n",
        "2. the `pl.LightningModule` class, which links optimizers to models and defines how the model behaves during training, validation, and testing.\n",
        "\n",
        "Both of these are kitted out with all the features\n",
        "a cutting-edge deep learning codebase needs:\n",
        "- flags for switching device types and distributed computing strategy\n",
        "- saving, checkpointing, and resumption\n",
        "- calculation and logging of metrics\n",
        "\n",
        "and much more.\n",
        "\n",
        "Importantly these features can be easily\n",
        "added, removed, extended, or bypassed\n",
        "as desired, meaning your code isn't constrained by the framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuJUDmCeT3RK"
      },
      "source": [
        "In some ways, you can think of Lightning as a tool for \"organizing\" your PyTorch code,\n",
        "as shown in the video below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wTt0TBs5TZpm"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"720\"\n",
              "            height=\"720\"\n",
              "            src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/pl_mod_vid.m4v\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x71b59f9529b0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import IPython.display as display\n",
        "\n",
        "\n",
        "display.IFrame(src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/pl_mod_vid.m4v\",\n",
        "               width=720, height=720)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGwpDn5GWn_X"
      },
      "source": [
        "That's opposed to the other way frameworks are designed,\n",
        "to provide abstractions over the lower-level library\n",
        "(here, PyTorch).\n",
        "\n",
        "Because of this \"organize don't abstract\" style,\n",
        "writing PyTorch Lightning code involves\n",
        "a lot of over-riding of methods --\n",
        "you inherit from a class\n",
        "and then implement the specific version of a general method\n",
        "that you need for your code,\n",
        "rather than Lightning providing a bunch of already\n",
        "fully-defined classes that you just instantiate,\n",
        "using arguments for configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXiUcQwan39S"
      },
      "source": [
        "# The `pl.LightningModule`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3FffD5Vn6we"
      },
      "source": [
        "The first of our two core classes,\n",
        "the `LightningModule`,\n",
        "is like a souped-up `torch.nn.Module` --\n",
        "it inherits all of the `Module` features,\n",
        "but adds more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0QWwSStJTP28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "issubclass(pl.LightningModule, torch.nn.Module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1wiBVSTuHNT"
      },
      "source": [
        "To demonstrate how this class works,\n",
        "we'll build up a `LinearRegression` model dynamically,\n",
        "method by method.\n",
        "\n",
        "For this example we hard code lots of the details,\n",
        "but the real benefit comes when the details are configurable.\n",
        "\n",
        "In order to have a realistic example as well,\n",
        "we'll compare to the actual code\n",
        "in the `BaseLitModel` we use in the codebase\n",
        "as we go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fPARncfQ3ohz"
      },
      "outputs": [],
      "source": [
        "from text_recognizer.lit_models import BaseLitModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myyL0vYU3z0a"
      },
      "source": [
        "A `pl.LightningModule` is a `torch.nn.Module`,\n",
        "so the basic definition looks the same:\n",
        "we need `__init__` and `forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-c0ylFO9rW_t"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()  # just like in torch.nn.Module, we need to call the parent class __init__\n",
        "\n",
        "        # attach torch.nn.Modules as top level attributes during init, just like in a torch.nn.Module\n",
        "        self.model = torch.nn.Linear(in_features=1, out_features=1)\n",
        "        # we like to define the entire model as one torch.nn.Module -- typically in a separate class\n",
        "\n",
        "    # optionally, define a forward method\n",
        "    def forward(self, xs):\n",
        "        return self.model(xs)  # we like to just call the model's forward method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY1yoGTy6CBu"
      },
      "source": [
        "But just the minimal definition for a `torch.nn.Module` isn't sufficient.\n",
        "\n",
        "If we try to use the class above with the `Trainer`, we get an error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tBWh_uHu5rmU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error:\n",
            "\tNo `training_step()` method defined. Lightning `Trainer` expects as minimum a\n",
            "\t`training_step()`, `train_dataloader()` and `configure_optimizers()` to be\n",
            "\tdefined.\n"
          ]
        }
      ],
      "source": [
        "import logging  # import some stdlib components to control what's display\n",
        "import textwrap\n",
        "import traceback\n",
        "\n",
        "\n",
        "try:  # try using the LinearRegression LightningModule defined above\n",
        "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)  # hide some info for now\n",
        "\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # we'll explain how the Trainer works in a bit\n",
        "    trainer = pl.Trainer(gpus=int(torch.cuda.is_available()), max_epochs=1)\n",
        "    trainer.fit(model=model)  \n",
        "\n",
        "except pl.utilities.exceptions.MisconfigurationException as error:\n",
        "    print(\"Error:\", *textwrap.wrap(str(error), 80), sep=\"\\n\\t\")  # show the error without raising it\n",
        "\n",
        "finally:  # bring back info-level logging\n",
        "    logging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5ni7xe5CgUt"
      },
      "source": [
        "The error message says we need some more methods.\n",
        "\n",
        "Two of them are mandatory components of the `LightningModule`: `.training_step` and `.configure_optimizers`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37BXP7nAoBik"
      },
      "source": [
        "#### `.training_step`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah9MjWz2plFv"
      },
      "source": [
        "The `training_step` method defines,\n",
        "naturally enough,\n",
        "what to do during a single step of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plWEvWG_zRia"
      },
      "source": [
        "Roughly, it gets used like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RbxZ4idy-C5"
      },
      "source": [
        "```python\n",
        "\n",
        "# pseudocode modified from the Lightning documentation\n",
        "\n",
        "# put model in train mode\n",
        "model.train()\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    # run the train step\n",
        "    loss = training_step(batch)\n",
        "\n",
        "    # clear gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # backprop\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cemh_hGJ53nL"
      },
      "source": [
        "Effectively, it maps a batch to a loss value,\n",
        "so that PyTorch can backprop through that loss.\n",
        "\n",
        "The `.training_step` for our `LinearRegression` model is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X8qW2VRRsPI2"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def training_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "    xs, ys = batch  # unpack the batch\n",
        "    outs = self(xs)  # apply the model\n",
        "    loss = torch.nn.functional.mse_loss(outs, ys)  # compute the (squared error) loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "LinearRegression.training_step = training_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2e8m3BRCIx6"
      },
      "source": [
        "If you've written PyTorch code before, you'll notice that we don't mention devices\n",
        "or other tensor metadata here -- that's handled for us by Lightning, which is a huge relief."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkvNpfwqpns5"
      },
      "source": [
        "You can additionally define\n",
        "a `validation_step` and a `test_step`\n",
        "to define the model's behavior during\n",
        "validation and testing loops.\n",
        "\n",
        "You're invited to define these steps\n",
        "in the exercises at the end of the lab.\n",
        "\n",
        "Inside this step is also where you might calculate other\n",
        "values related to inputs, outputs, and loss,\n",
        "like non-differentiable metrics (e.g. accuracy, precision, recall).\n",
        "\n",
        "So our `BaseLitModel`'s got a slightly more complex `training_step` method,\n",
        "and the details of the forward pass are deferred to `._run_on_batch` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xpBkRczao1hr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mSignature:\u001b[0m \u001b[0mBaseLitModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDocstring:\u001b[0m\n",
            "Here you compute and return the training loss and some additional metrics for e.g.\n",
            "the progress bar or logger.\n",
            "\n",
            "Args:\n",
            "    batch (:class:`~torch.Tensor` | (:class:`~torch.Tensor`, ...) | [:class:`~torch.Tensor`, ...]):\n",
            "        The output of your :class:`~torch.utils.data.DataLoader`. A tensor, tuple or list.\n",
            "    batch_idx (``int``): Integer displaying index of this batch\n",
            "    optimizer_idx (``int``): When using multiple optimizers, this argument will also be present.\n",
            "    hiddens (``Any``): Passed in if\n",
            "        :paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps` > 0.\n",
            "\n",
            "Return:\n",
            "    Any of.\n",
            "\n",
            "    - :class:`~torch.Tensor` - The loss tensor\n",
            "    - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'``\n",
            "    - ``None`` - Training will skip to the next batch. This is only for automatic optimization.\n",
            "        This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.\n",
            "\n",
            "In this step you'd normally do the forward pass and calculate the loss for a batch.\n",
            "You can also do fancier things like multiple forward passes or something model specific.\n",
            "\n",
            "Example::\n",
            "\n",
            "    def training_step(self, batch, batch_idx):\n",
            "        x, y, z = batch\n",
            "        out = self.encoder(x)\n",
            "        loss = self.loss(out, x)\n",
            "        return loss\n",
            "\n",
            "If you define multiple optimizers, this step will be called with an additional\n",
            "``optimizer_idx`` parameter.\n",
            "\n",
            ".. code-block:: python\n",
            "\n",
            "    # Multiple optimizers (e.g.: GANs)\n",
            "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
            "        if optimizer_idx == 0:\n",
            "            # do training_step with encoder\n",
            "            ...\n",
            "        if optimizer_idx == 1:\n",
            "            # do training_step with decoder\n",
            "            ...\n",
            "\n",
            "\n",
            "If you add truncated back propagation through time you will also get an additional\n",
            "argument with the hidden states of the previous step.\n",
            "\n",
            ".. code-block:: python\n",
            "\n",
            "    # Truncated back-propagation through time\n",
            "    def training_step(self, batch, batch_idx, hiddens):\n",
            "        # hiddens are the hidden states from the previous truncated backprop step\n",
            "        out, hiddens = self.lstm(data, hiddens)\n",
            "        loss = ...\n",
            "        return {\"loss\": loss, \"hiddens\": hiddens}\n",
            "\n",
            "Note:\n",
            "    The loss value shown in the progress bar is smoothed (averaged) over the last values,\n",
            "    so it differs from the actual loss returned in train/validation step.\n",
            "\u001b[0;31mSource:\u001b[0m   \n",
            "    \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train/loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train/acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFile:\u001b[0m      ~/fsdl-text-recognizer-2022-labs/lab02/text_recognizer/lit_models/base.py\n",
            "\u001b[0;31mType:\u001b[0m      function"
          ]
        }
      ],
      "source": [
        "BaseLitModel.training_step??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guhoYf_NoEyc"
      },
      "source": [
        "#### `.configure_optimizers`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCIAWoCEtIU7"
      },
      "source": [
        "Thanks to `training_step` we've got a loss, and PyTorch can turn that into a gradient.\n",
        "\n",
        "But we need more than a gradient to do an update.\n",
        "\n",
        "We need an _optimizer_ that can make use of the gradients to update the parameters. In complex cases, we might need more than one optimizer (e.g. GANs).\n",
        "\n",
        "Our second required method, `.configure_optimizers`,\n",
        "sets up the `torch.optim.Optimizer`s \n",
        "(e.g. setting their hyperparameters\n",
        "and pointing them at the `Module`'s parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMlnRdIPzvDF"
      },
      "source": [
        "In psuedo-code (modified from the Lightning documentation), it gets used something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WBnfJzszi49"
      },
      "source": [
        "```python\n",
        "optimizer = model.configure_optimizers()\n",
        "\n",
        "for batch_idx, batch in enumerate(data):\n",
        "\n",
        "    def closure():  # wrap the loss calculation\n",
        "        loss = model.training_step(batch, batch_idx, ...)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    # optimizer can call the loss calculation as many times as it likes\n",
        "    optimizer.step(closure)  # some optimizers need this, like (L)-BFGS\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGsP3DBy7YzW"
      },
      "source": [
        "For our `LinearRegression` model,\n",
        "we just need to instantiate an optimizer and point it at the parameters of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZWrWGgdVt21h"
      },
      "outputs": [],
      "source": [
        "def configure_optimizers(self: LinearRegression) -> torch.optim.Optimizer:\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=3e-4)  # https://fsdl.me/ol-reliable-img\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "LinearRegression.configure_optimizers = configure_optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta2hs0OLwbtF"
      },
      "source": [
        "You can read more about optimization in Lightning,\n",
        "including how to manually control optimization\n",
        "instead of relying on default behavior,\n",
        "in the docs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KXINqlAgwfKy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://pytorch-lightning.readthedocs.io/en/1.6.3/common/optimization.html'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimization_docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/common/optimization.html\"\n",
        "optimization_docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWdKdZDfxmb2"
      },
      "source": [
        "The `configure_optimizers` method for the `BaseLitModel`\n",
        "isn't that much more complex.\n",
        "\n",
        "We just add support for learning rate schedulers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kyRbz0bEpWwd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mSignature:\u001b[0m \u001b[0mBaseLitModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDocstring:\u001b[0m\n",
            "Choose what optimizers and learning-rate schedulers to use in your optimization.\n",
            "Normally you'd need one. But in the case of GANs or similar you might have multiple.\n",
            "\n",
            "Return:\n",
            "    Any of these 6 options.\n",
            "\n",
            "    - **Single optimizer**.\n",
            "    - **List or Tuple** of optimizers.\n",
            "    - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers\n",
            "      (or multiple ``lr_scheduler_config``).\n",
            "    - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"``\n",
            "      key whose value is a single LR scheduler or ``lr_scheduler_config``.\n",
            "    - **Tuple of dictionaries** as described above, with an optional ``\"frequency\"`` key.\n",
            "    - **None** - Fit will run without any optimizer.\n",
            "\n",
            "The ``lr_scheduler_config`` is a dictionary which contains the scheduler and its associated configuration.\n",
            "The default configuration is shown below.\n",
            "\n",
            ".. code-block:: python\n",
            "\n",
            "    lr_scheduler_config = {\n",
            "        # REQUIRED: The scheduler instance\n",
            "        \"scheduler\": lr_scheduler,\n",
            "        # The unit of the scheduler's step size, could also be 'step'.\n",
            "        # 'epoch' updates the scheduler on epoch end whereas 'step'\n",
            "        # updates it after a optimizer update.\n",
            "        \"interval\": \"epoch\",\n",
            "        # How many epochs/steps should pass between calls to\n",
            "        # `scheduler.step()`. 1 corresponds to updating the learning\n",
            "        # rate after every epoch/step.\n",
            "        \"frequency\": 1,\n",
            "        # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
            "        \"monitor\": \"val_loss\",\n",
            "        # If set to `True`, will enforce that the value specified 'monitor'\n",
            "        # is available when the scheduler is updated, thus stopping\n",
            "        # training if not found. If set to `False`, it will only produce a warning\n",
            "        \"strict\": True,\n",
            "        # If using the `LearningRateMonitor` callback to monitor the\n",
            "        # learning rate progress, this keyword can be used to specify\n",
            "        # a custom logged name\n",
            "        \"name\": None,\n",
            "    }\n",
            "\n",
            "When there are schedulers in which the ``.step()`` method is conditioned on a value, such as the\n",
            ":class:`torch.optim.lr_scheduler.ReduceLROnPlateau` scheduler, Lightning requires that the\n",
            "``lr_scheduler_config`` contains the keyword ``\"monitor\"`` set to the metric name that the scheduler\n",
            "should be conditioned on.\n",
            "\n",
            ".. testcode::\n",
            "\n",
            "    # The ReduceLROnPlateau scheduler requires a monitor\n",
            "    def configure_optimizers(self):\n",
            "        optimizer = Adam(...)\n",
            "        return {\n",
            "            \"optimizer\": optimizer,\n",
            "            \"lr_scheduler\": {\n",
            "                \"scheduler\": ReduceLROnPlateau(optimizer, ...),\n",
            "                \"monitor\": \"metric_to_track\",\n",
            "                \"frequency\": \"indicates how often the metric is updated\"\n",
            "                # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\n",
            "                # multiple of \"trainer.check_val_every_n_epoch\".\n",
            "            },\n",
            "        }\n",
            "\n",
            "\n",
            "    # In the case of two optimizers, only one using the ReduceLROnPlateau scheduler\n",
            "    def configure_optimizers(self):\n",
            "        optimizer1 = Adam(...)\n",
            "        optimizer2 = SGD(...)\n",
            "        scheduler1 = ReduceLROnPlateau(optimizer1, ...)\n",
            "        scheduler2 = LambdaLR(optimizer2, ...)\n",
            "        return (\n",
            "            {\n",
            "                \"optimizer\": optimizer1,\n",
            "                \"lr_scheduler\": {\n",
            "                    \"scheduler\": scheduler1,\n",
            "                    \"monitor\": \"metric_to_track\",\n",
            "                },\n",
            "            },\n",
            "            {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2},\n",
            "        )\n",
            "\n",
            "Metrics can be made available to monitor by simply logging it using\n",
            "``self.log('metric_to_track', metric_val)`` in your :class:`~pytorch_lightning.core.lightning.LightningModule`.\n",
            "\n",
            "Note:\n",
            "    The ``frequency`` value specified in a dict along with the ``optimizer`` key is an int corresponding\n",
            "    to the number of sequential batches optimized with the specific optimizer.\n",
            "    It should be given to none or to all of the optimizers.\n",
            "    There is a difference between passing multiple optimizers in a list,\n",
            "    and passing multiple optimizers in dictionaries with a frequency of 1:\n",
            "\n",
            "        - In the former case, all optimizers will operate on the given batch in each optimization step.\n",
            "        - In the latter, only one optimizer will operate on the given batch at every step.\n",
            "\n",
            "    This is different from the ``frequency`` value specified in the ``lr_scheduler_config`` mentioned above.\n",
            "\n",
            "    .. code-block:: python\n",
            "\n",
            "        def configure_optimizers(self):\n",
            "            optimizer_one = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
            "            optimizer_two = torch.optim.SGD(self.model.parameters(), lr=0.01)\n",
            "            return [\n",
            "                {\"optimizer\": optimizer_one, \"frequency\": 5},\n",
            "                {\"optimizer\": optimizer_two, \"frequency\": 10},\n",
            "            ]\n",
            "\n",
            "    In this example, the first optimizer will be used for the first 5 steps,\n",
            "    the second optimizer for the next 10 steps and that cycle will continue.\n",
            "    If an LR scheduler is specified for an optimizer using the ``lr_scheduler`` key in the above dict,\n",
            "    the scheduler will only be updated when its optimizer is being used.\n",
            "\n",
            "Examples::\n",
            "\n",
            "    # most cases. no learning rate scheduler\n",
            "    def configure_optimizers(self):\n",
            "        return Adam(self.parameters(), lr=1e-3)\n",
            "\n",
            "    # multiple optimizer case (e.g.: GAN)\n",
            "    def configure_optimizers(self):\n",
            "        gen_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
            "        dis_opt = Adam(self.model_dis.parameters(), lr=0.02)\n",
            "        return gen_opt, dis_opt\n",
            "\n",
            "    # example with learning rate schedulers\n",
            "    def configure_optimizers(self):\n",
            "        gen_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
            "        dis_opt = Adam(self.model_dis.parameters(), lr=0.02)\n",
            "        dis_sch = CosineAnnealing(dis_opt, T_max=10)\n",
            "        return [gen_opt, dis_opt], [dis_sch]\n",
            "\n",
            "    # example with step-based learning rate schedulers\n",
            "    # each optimizer has its own scheduler\n",
            "    def configure_optimizers(self):\n",
            "        gen_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
            "        dis_opt = Adam(self.model_dis.parameters(), lr=0.02)\n",
            "        gen_sch = {\n",
            "            'scheduler': ExponentialLR(gen_opt, 0.99),\n",
            "            'interval': 'step'  # called after each training step\n",
            "        }\n",
            "        dis_sch = CosineAnnealing(dis_opt, T_max=10) # called every epoch\n",
            "        return [gen_opt, dis_opt], [gen_sch, dis_sch]\n",
            "\n",
            "    # example with optimizer frequencies\n",
            "    # see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1\n",
            "    # https://arxiv.org/abs/1704.00028\n",
            "    def configure_optimizers(self):\n",
            "        gen_opt = Adam(self.model_gen.parameters(), lr=0.01)\n",
            "        dis_opt = Adam(self.model_dis.parameters(), lr=0.02)\n",
            "        n_critic = 5\n",
            "        return (\n",
            "            {'optimizer': dis_opt, 'frequency': n_critic},\n",
            "            {'optimizer': gen_opt, 'frequency': 1}\n",
            "        )\n",
            "\n",
            "Note:\n",
            "    Some things to know:\n",
            "\n",
            "    - Lightning calls ``.backward()`` and ``.step()`` on each optimizer and learning rate scheduler as needed.\n",
            "    - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizers.\n",
            "    - If you use multiple optimizers, :meth:`training_step` will have an additional ``optimizer_idx`` parameter.\n",
            "    - If you use :class:`torch.optim.LBFGS`, Lightning handles the closure function automatically for you.\n",
            "    - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer\n",
            "      at each training step.\n",
            "    - If you need to control how often those optimizers step or override the default ``.step()`` schedule,\n",
            "      override the :meth:`optimizer_step` hook.\n",
            "\u001b[0;31mSource:\u001b[0m   \n",
            "    \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_cycle_max_lr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneCycleLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_cycle_max_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_cycle_total_steps\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr_scheduler\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"monitor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"validation/loss\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFile:\u001b[0m      ~/fsdl-text-recognizer-2022-labs/lab02/text_recognizer/lit_models/base.py\n",
            "\u001b[0;31mType:\u001b[0m      function"
          ]
        }
      ],
      "source": [
        "BaseLitModel.configure_optimizers??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilQCfn7Nm_QP"
      },
      "source": [
        "# The `pl.Trainer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RScc0ef97qlc"
      },
      "source": [
        "The `LightningModule` has already helped us organize our code,\n",
        "but it's not really useful until we combine it with the `Trainer`,\n",
        "which relies on the `LightningModule` interface to execute training, validation, and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBdikPBF86Qp"
      },
      "source": [
        "The `Trainer` is where we make choices like how long to train\n",
        "(`max_epochs`, `min_epochs`, `max_time`, `max_steps`),\n",
        "what kind of acceleration (e.g. `gpus`) or distribution strategy to use,\n",
        "and other settings that might differ across training runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "int(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YQ4KSdFP3E4Q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(max_epochs=20, gpus=int(torch.cuda.is_available()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2l3rGZK7-PL"
      },
      "source": [
        "Before we can actually use the `Trainer`, though,\n",
        "we also need a `torch.utils.data.DataLoader` --\n",
        "nothing new from PyTorch Lightning here,\n",
        "just vanilla PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OcUSD2jP4Ffo"
      },
      "outputs": [],
      "source": [
        "class CorrelatedDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, N=10_000):\n",
        "        self.N = N\n",
        "        self.xs = torch.randn(size=(N, 1))\n",
        "        self.ys = torch.randn_like(self.xs) + self.xs  # correlated target data: y ~ N(x, 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.xs[idx], self.ys[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.N\n",
        "\n",
        "\n",
        "dataset = CorrelatedDataset()\n",
        "tdl = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0u41JtA8qGo"
      },
      "source": [
        "We can fetch some sample data from the `DataLoader`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "z1j6Gj9Ka0dJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs:\n",
            "tensor([[-0.7747],\n",
            "        [ 0.5762],\n",
            "        [-0.6215],\n",
            "        [ 2.2223],\n",
            "        [ 0.1277],\n",
            "        [ 0.8216],\n",
            "        [-1.1073],\n",
            "        [ 0.1728],\n",
            "        [ 0.1325],\n",
            "        [ 0.1224]])\n",
            "ys:\n",
            "tensor([[ 0.1048],\n",
            "        [ 0.6417],\n",
            "        [-0.6389],\n",
            "        [ 2.8478],\n",
            "        [ 0.5374],\n",
            "        [-0.6872],\n",
            "        [-0.6125],\n",
            "        [ 0.9905],\n",
            "        [-1.4590],\n",
            "        [ 0.3950]])\n"
          ]
        }
      ],
      "source": [
        "example_xs, example_ys = next(iter(tdl))  # grabbing an example batch to print\n",
        "\n",
        "print(\"xs:\", example_xs[:10], sep=\"\\n\")\n",
        "print(\"ys:\", example_ys[:10], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnqk3mRv8dbW"
      },
      "source": [
        "and, since it's low-dimensional, visualize it\n",
        "and see what we're asking the model to learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "33jcHbErbl6Q"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "pd.DataFrame(data={\"x\": example_xs.flatten(), \"y\": example_ys.flatten()})\\\n",
        "  .plot(x=\"x\", y=\"y\", kind=\"scatter\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA7-4tJJ9fde"
      },
      "source": [
        "Now we're ready to run training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IY910O803oPU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss before training: 1.0049773454666138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type   | Params\n",
            "---------------------------------\n",
            "0 | model | Linear | 2     \n",
            "---------------------------------\n",
            "2         Trainable params\n",
            "0         Non-trainable params\n",
            "2         Total params\n",
            "0.000     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1527cd6690c143f2a8695269fc311b03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss after training: 1.0037803649902344\n"
          ]
        }
      ],
      "source": [
        "model = LinearRegression()\n",
        "\n",
        "print(\"loss before training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())\n",
        "\n",
        "trainer.fit(model=model, train_dataloaders=tdl)\n",
        "\n",
        "print(\"loss after training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQBXYmLF_GoI"
      },
      "source": [
        "The loss after training should be less than the loss before training,\n",
        "and we can see that our model's predictions line up with the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jqcbA91x96-s"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ax = pd.DataFrame(data={\"x\": example_xs.flatten(), \"y\": example_ys.flatten()})\\\n",
        "  .plot(x=\"x\", y=\"y\", legend=True, kind=\"scatter\", label=\"data\")\n",
        "\n",
        "inps = torch.arange(-2, 2, 0.5)[:, None]\n",
        "ax.plot(inps, model(inps).detach(), lw=2, color=\"k\", label=\"predictions\"); ax.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZkpsNfl3P8R"
      },
      "source": [
        "The `Trainer` promises to \"customize every aspect of training via flags\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_Q-c9b62_XFj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Customize every aspect of training via flags.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pl.Trainer.__init__.__doc__.strip().split(\"\\n\")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He-zEwMB_oKH"
      },
      "source": [
        "and they mean _every_ aspect.\n",
        "\n",
        "The cell below prints all of the arguments for the `pl.Trainer` class --\n",
        "no need to memorize or even understand them all now,\n",
        "just skim it to see how many customization options there are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8F_rRPL3lfPE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "        Customize every aspect of training via flags.\n",
            "\n",
            "        Args:\n",
            "\n",
            "            accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\")\n",
            "                as well as custom accelerator instances.\n",
            "\n",
            "                .. deprecated:: v1.5\n",
            "                    Passing training strategies (e.g., 'ddp') to ``accelerator`` has been deprecated in v1.5.0\n",
            "                    and will be removed in v1.7.0. Please use the ``strategy`` argument instead.\n",
            "\n",
            "            accumulate_grad_batches: Accumulates grads every k batches or as set up in the dict.\n",
            "                Default: ``None``.\n",
            "\n",
            "            amp_backend: The mixed precision backend to use (\"native\" or \"apex\").\n",
            "                Default: ``'native''``.\n",
            "\n",
            "            amp_level: The optimization level to use (O1, O2, etc...). By default it will be set to \"O2\"\n",
            "                if ``amp_backend`` is set to \"apex\".\n",
            "\n",
            "            auto_lr_find: If set to True, will make trainer.tune() run a learning rate finder,\n",
            "                trying to optimize initial learning for faster convergence. trainer.tune() method will\n",
            "                set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.\n",
            "                To use a different key set a string instead of True with the key name.\n",
            "                Default: ``False``.\n",
            "\n",
            "            auto_scale_batch_size: If set to True, will `initially` run a batch size\n",
            "                finder trying to find the largest batch size that fits into memory.\n",
            "                The result will be stored in self.batch_size in the LightningModule.\n",
            "                Additionally, can be set to either `power` that estimates the batch size through\n",
            "                a power search or `binsearch` that estimates the batch size through a binary search.\n",
            "                Default: ``False``.\n",
            "\n",
            "            auto_select_gpus: If enabled and ``gpus`` or ``devices`` is an integer, pick available\n",
            "                gpus automatically. This is especially useful when\n",
            "                GPUs are configured to be in \"exclusive mode\", such\n",
            "                that only one process at a time can access them.\n",
            "                Default: ``False``.\n",
            "\n",
            "            benchmark: Sets ``torch.backends.cudnn.benchmark``.\n",
            "                Defaults to ``True`` if :paramref:`~pytorch_lightning.trainer.trainer.Trainer.deterministic`\n",
            "                is ``False``. Overwrite to manually set a different value. Default: ``None``.\n",
            "\n",
            "            callbacks: Add a callback or list of callbacks.\n",
            "                Default: ``None``.\n",
            "\n",
            "            checkpoint_callback: If ``True``, enable checkpointing.\n",
            "                Default: ``None``.\n",
            "\n",
            "                .. deprecated:: v1.5\n",
            "                    ``checkpoint_callback`` has been deprecated in v1.5 and will be removed in v1.7.\n",
            "                    Please consider using ``enable_checkpointing`` instead.\n",
            "\n",
            "            enable_checkpointing: If ``True``, enable checkpointing.\n",
            "                It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
            "                :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.\n",
            "                Default: ``True``.\n",
            "\n",
            "            check_val_every_n_epoch: Check val every n train epochs.\n",
            "                Default: ``1``.\n",
            "\n",
            "\n",
            "            default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
            "                Default: ``os.getcwd()``.\n",
            "                Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
            "\n",
            "            detect_anomaly: Enable anomaly detection for the autograd engine.\n",
            "                Default: ``False``.\n",
            "\n",
            "            deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\n",
            "                Default: ``False``.\n",
            "\n",
            "            devices: Will be mapped to either `gpus`, `tpu_cores`, `num_processes` or `ipus`,\n",
            "                based on the accelerator type.\n",
            "\n",
            "            fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
            "                of train, val and test to find any bugs (ie: a sort of unit test).\n",
            "                Default: ``False``.\n",
            "\n",
            "            flush_logs_every_n_steps: How often to flush logs to disk (defaults to every 100 steps).\n",
            "\n",
            "                .. deprecated:: v1.5\n",
            "                    ``flush_logs_every_n_steps`` has been deprecated in v1.5 and will be removed in v1.7.\n",
            "                    Please configure flushing directly in the logger instead.\n",
            "\n",
            "            gpus: Number of GPUs to train on (int) or which GPUs to train on (list or str) applied per node\n",
            "                Default: ``None``.\n",
            "\n",
            "            gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\n",
            "                gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\n",
            "                Default: ``None``.\n",
            "\n",
            "            gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\n",
            "                to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\n",
            "                be set to ``\"norm\"``.\n",
            "\n",
            "            limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\n",
            "                Default: ``1.0``.\n",
            "\n",
            "            limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\n",
            "                Default: ``1.0``.\n",
            "\n",
            "            limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\n",
            "                Default: ``1.0``.\n",
            "\n",
            "            limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\n",
            "                Default: ``1.0``.\n",
            "\n",
            "            logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n",
            "                the default ``TensorBoardLogger``. ``False`` will disable logging. If multiple loggers are\n",
            "                provided and the `save_dir` property of that logger is not set, local files (checkpoints,\n",
            "                profiler traces, etc.) are saved in ``default_root_dir`` rather than in the ``log_dir`` of any\n",
            "                of the individual loggers.\n",
            "                Default: ``True``.\n",
            "\n",
            "            log_gpu_memory: None, 'min_max', 'all'. Might slow performance.\n",
            "\n",
            "                .. deprecated:: v1.5\n",
            "                    Deprecated in v1.5.0 and will be removed in v1.7.0\n",
            "                    Please use the ``DeviceStatsMonitor`` callback directly instead.\n",
            "\n",
            "            log_every_n_steps: How often to log within steps.\n",
            "                Default: ``50``.\n",
            "\n",
            "            prepare_data_per_node: If True, each LOCAL_RANK=0 will call prepare data.\n",
            "                Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data\n",
            "\n",
            "                .. deprecated:: v1.5\n",
            "                    Deprecated in v1.5.0 and will be removed in v1.7.0\n",
            "                    Please set ``prepare_data_per_node`` in ``LightningDataModule`` and/or\n",
            "                    ``LightningModule`` directly instead.\n",
            "\n",
            "            process_position: Orders the progress bar when running multiple models on same machine.\n",
            "\n",
            "                .. deprecated:: v1.5\n",
            "                    ``process_position`` has been deprecated in v1.5 and will be removed in v1.7.\n",
            "                    Please pass :class:`~pytorch_lightning.callbacks.progress.TQDMProgressBar` with ``process_position``\n",
            "                    directly to the Trainer's ``callbacks`` argument instead.\n",
            "\n",
            "            progress_bar_refresh_rate: How often to refresh progress bar (in steps). Value ``0`` disables progress bar.\n",
            "                Ignored when a custom progress bar is passed to :paramref:`~Trainer.callbacks`. Default: None, means\n",
            "                a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).\n",
            "\n",
            "                .. deprecated:: v1.5\n",
            "                    ``progress_bar_refresh_rate`` has been deprecated in v1.5 and will be removed in v1.7.\n",
            "                    Please pass :class:`~pytorch_lightning.callbacks.progress.TQDMProgressBar` with ``refresh_rate``\n",
            "                    directly to the Trainer's ``callbacks`` argument instead. To disable the progress bar,\n",
            "                    pass ``enable_progress_bar = False`` to the Trainer.\n",
            "\n",
            "            enable_progress_bar: Whether to enable to progress bar by default.\n",
            "                Default: ``False``.\n",
            "\n",
            "            profiler: To profile individual steps during training and assist in identifying bottlenecks.\n",
            "                Default: ``None``.\n",
            "\n",
            "            overfit_batches: Overfit a fraction of training data (float) or a set number of batches (int).\n",
            "                Default: ``0.0``.\n",
            "\n",
            "            plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
            "                Default: ``None``.\n",
            "\n",
            "            precision: Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16).\n",
            "                Can be used on CPU, GPU, TPUs, HPUs or IPUs.\n",
            "                Default: ``32``.\n",
            "\n",
            "            max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
            "                If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n",
            "                To enable infinite training, set ``max_epochs = -1``.\n",
            "\n",
            "            min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
            "\n",
            "            max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n",
            "                and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n",
            "                ``max_epochs`` to ``-1``.\n",
            "\n",
            "            min_steps: Force training for at least these number of steps. Disabled by default (``None``).\n",
            "\n",
            "            max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\n",
            "                The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\n",
            "                :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\n",
            "                :class:`datetime.timedelta`.\n",
            "\n",
            "            num_nodes: Number of GPU nodes for distributed training.\n",
            "                Default: ``1``.\n",
            "\n",
            "            num_processes: Number of processes for distributed training with ``accelerator=\"cpu\"``.\n",
            "                Default: ``1``.\n",
            "\n",
            "            num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
            "                Set it to `-1` to run all batches in all validation dataloaders.\n",
            "                Default: ``2``.\n",
            "\n",
            "            reload_dataloaders_every_n_epochs: Set to a non-negative integer to reload dataloaders every n epochs.\n",
            "                Default: ``0``.\n",
            "\n",
            "            replace_sampler_ddp: Explicitly enables or disables sampler replacement. If not specified this\n",
            "                will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for\n",
            "                train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it,\n",
            "                you can set ``replace_sampler_ddp=False`` and add your own distributed sampler.\n",
            "\n",
            "            resume_from_checkpoint: Path/URL of the checkpoint from which training is resumed. If there is\n",
            "                no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint,\n",
            "                training will start from the beginning of the next epoch.\n",
            "\n",
            "                .. deprecated:: v1.5\n",
            "                    ``resume_from_checkpoint`` is deprecated in v1.5 and will be removed in v2.0.\n",
            "                    Please pass the path to ``Trainer.fit(..., ckpt_path=...)`` instead.\n",
            "\n",
            "            strategy: Supports different training strategies with aliases\n",
            "                as well custom strategies.\n",
            "                Default: ``None``.\n",
            "\n",
            "            sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
            "                Default: ``False``.\n",
            "\n",
            "            terminate_on_nan: If set to True, will terminate training (by raising a `ValueError`) at the\n",
            "                end of each training batch, if any of the parameters or the loss are NaN or +/-inf.\n",
            "\n",
            "                .. deprecated:: v1.5\n",
            "                    Trainer argument ``terminate_on_nan`` was deprecated in v1.5 and will be removed in 1.7.\n",
            "                    Please use ``detect_anomaly`` instead.\n",
            "\n",
            "            detect_anomaly: Enable anomaly detection for the autograd engine.\n",
            "                Default: ``False``.\n",
            "\n",
            "            tpu_cores: How many TPU cores to train on (1 or 8) / Single TPU to train on (1)\n",
            "                Default: ``None``.\n",
            "\n",
            "            ipus: How many IPUs to train on.\n",
            "                Default: ``None``.\n",
            "\n",
            "            track_grad_norm: -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm. If using\n",
            "                Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them.\n",
            "                Default: ``-1``.\n",
            "\n",
            "            val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\n",
            "                after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\n",
            "                batches.\n",
            "                Default: ``1.0``.\n",
            "\n",
            "            enable_model_summary: Whether to enable model summarization by default.\n",
            "                Default: ``True``.\n",
            "\n",
            "            weights_summary: Prints a summary of the weights when training begins.\n",
            "\n",
            "                .. deprecated:: v1.5\n",
            "                    ``weights_summary`` has been deprecated in v1.5 and will be removed in v1.7.\n",
            "                    To disable the summary, pass ``enable_model_summary = False`` to the Trainer.\n",
            "                    To customize the summary, pass :class:`~pytorch_lightning.callbacks.model_summary.ModelSummary`\n",
            "                    directly to the Trainer's ``callbacks`` argument.\n",
            "\n",
            "            weights_save_path: Where to save weights if specified. Will override default_root_dir\n",
            "                for checkpoints only. Use this if for whatever reason you need the checkpoints\n",
            "                stored in a different place than the logs written in `default_root_dir`.\n",
            "                Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
            "                Defaults to `default_root_dir`.\n",
            "\n",
            "                .. deprecated:: v1.6\n",
            "                    ``weights_save_path`` has been deprecated in v1.6 and will be removed in v1.8. Please pass\n",
            "                    ``dirpath`` directly to the :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint`\n",
            "                    callback.\n",
            "\n",
            "            move_metrics_to_cpu: Whether to force internal logged metrics to be moved to cpu.\n",
            "                This can save some gpu memory, but can make training slower. Use with attention.\n",
            "                Default: ``False``.\n",
            "\n",
            "            multiple_trainloader_mode: How to loop over the datasets when there are multiple train loaders.\n",
            "                In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,\n",
            "                and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets\n",
            "                reload when reaching the minimum length of datasets.\n",
            "                Default: ``\"max_size_cycle\"``.\n",
            "\n",
            "            stochastic_weight_avg: Whether to use `Stochastic Weight Averaging (SWA)\n",
            "                <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>`_.\n",
            "                Default: ``False``.\n",
            "\n",
            "                .. deprecated:: v1.5\n",
            "                    ``stochastic_weight_avg`` has been deprecated in v1.5 and will be removed in v1.7.\n",
            "                    Please pass :class:`~pytorch_lightning.callbacks.stochastic_weight_avg.StochasticWeightAveraging`\n",
            "                    directly to the Trainer's ``callbacks`` argument instead.\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "print(pl.Trainer.__init__.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X8dGmR53kYU"
      },
      "source": [
        "It's probably easier to read them on the documentation website:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cqUj6MxRkppr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://pytorch-lightning.readthedocs.io/en/1.6.3/common/trainer.html'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer_docs_link = f\"https://pytorch-lightning.readthedocs.io/en/{version}/common/trainer.html\"\n",
        "trainer_docs_link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T8XMYvr__Y5"
      },
      "source": [
        "# Training with PyTorch Lightning in the FSDL Codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CtaPliTAxy3"
      },
      "source": [
        "The `LightningModule`s in the FSDL codebase\n",
        "are stored in the `lit_models` submodule of the `text_recognizer` module.\n",
        "\n",
        "For now, we've just got some basic models.\n",
        "We'll add more as we go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NMe5z1RSAyo_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__init__.py  __pycache__  base.py\n"
          ]
        }
      ],
      "source": [
        "!ls text_recognizer/lit_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZTYmIHbBu7g"
      },
      "source": [
        "We also have a folder called `training` now.\n",
        "\n",
        "This contains a script, `run_experiment.py`,\n",
        "that is used for running training jobs.\n",
        "\n",
        "In case you want to play around with the training code\n",
        "in a notebook, you can also load it as a module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DRz9GbXzNJLM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__init__.py  __pycache__  run_experiment.py  util.py\n"
          ]
        }
      ],
      "source": [
        "!ls training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Im9vLeyqBv_h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment-running framework. \n",
            "    Run an experiment.\n",
            "\n",
            "    Sample command:\n",
            "    ```\n",
            "    python training/run_experiment.py --max_epochs=3 --gpus='0,' --num_workers=20 --model_class=MLP --data_class=MNIST\n",
            "    ```\n",
            "\n",
            "    For basic help documentation, run the command\n",
            "    ```\n",
            "    python training/run_experiment.py --help\n",
            "    ```\n",
            "\n",
            "    The available command line args differ depending on some of the arguments, including --model_class and --data_class.\n",
            "\n",
            "    To see which command line args are available and read their documentation, provide values for those arguments\n",
            "    before invoking --help, like so:\n",
            "    ```\n",
            "    python training/run_experiment.py --model_class=MLP --data_class=MNIST --help\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import training.run_experiment\n",
        "\n",
        "\n",
        "print(training.run_experiment.__doc__, training.run_experiment.main.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2hcAXqHAV0v"
      },
      "source": [
        "We build the `Trainer` from command line arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "yi50CDZul7Mm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    trainer = pl.Trainer.from_argparse_args(args, callbacks=callbacks, logger=logger)\n"
          ]
        }
      ],
      "source": [
        "# how the trainer is initialized in the training script\n",
        "!grep \"pl.Trainer.from\" training/run_experiment.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQheYJyAxlh"
      },
      "source": [
        "so all the configuration flexibility and complexity of the `Trainer`\n",
        "is available via the command line.\n",
        "\n",
        "Docs for the command line arguments for the trainer are accessible with `--help`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XlSmSyCMAw7Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pl.Trainer:\n",
            "  --logger [LOGGER]     Logger (or iterable collection of loggers) for\n",
            "                        experiment tracking. A ``True`` value uses the default\n",
            "                        ``TensorBoardLogger``. ``False`` will disable logging.\n",
            "                        If multiple loggers are provided and the `save_dir`\n",
            "                        property of that logger is not set, local files\n",
            "                        (checkpoints, profiler traces, etc.) are saved in\n",
            "                        ``default_root_dir`` rather than in the ``log_dir`` of\n",
            "                        any of the individual loggers. Default: ``True``.\n",
            "  --checkpoint_callback [CHECKPOINT_CALLBACK]\n",
            "                        If ``True``, enable checkpointing. Default: ``None``.\n",
            "                        .. deprecated:: v1.5 ``checkpoint_callback`` has been\n",
            "                        deprecated in v1.5 and will be removed in v1.7. Please\n",
            "                        consider using ``enable_checkpointing`` instead.\n",
            "  --enable_checkpointing [ENABLE_CHECKPOINTING]\n",
            "                        If ``True``, enable checkpointing. It will configure a\n",
            "                        default ModelCheckpoint callback if there is no user-\n",
            "                        defined ModelCheckpoint in :paramref:`~pytorch_lightni\n",
            "                        ng.trainer.trainer.Trainer.callbacks`. Default:\n",
            "                        ``True``.\n",
            "  --default_root_dir DEFAULT_ROOT_DIR\n",
            "                        Default path for logs and weights when no\n",
            "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\n",
            "                        Can be remote file paths such as `s3://mybucket/path`\n",
            "                        or 'hdfs://path/'\n"
          ]
        }
      ],
      "source": [
        "# displays the first few flags for controlling the Trainer from the command line\n",
        "!python training/run_experiment.py --help | grep \"pl.Trainer\" -A 24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIZ_VRPcNMsM"
      },
      "source": [
        "We'll use `run_experiment` in\n",
        "[Lab 02b](http://fsdl.me/lab02b-colab)\n",
        "to train convolutional neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0siaL4Qumc_"
      },
      "source": [
        "# Extra Goodies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkQSPnxQDBF6"
      },
      "source": [
        "The `LightningModule` and the `Trainer` are the minimum amount you need\n",
        "to get started with PyTorch Lightning.\n",
        "\n",
        "But they aren't all you need.\n",
        "\n",
        "There are many more features built into Lightning and its ecosystem.\n",
        "\n",
        "We'll cover three more here:\n",
        "- `pl.LightningDataModule`s, for organizing dataloaders and handling data in distributed settings\n",
        "- `pl.Callback`s, for adding \"optional\" extra features to model training\n",
        "- `torchmetrics`, for efficiently computing and logging "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOYHSLw_D8Zy"
      },
      "source": [
        "## `pl.LightningDataModule`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpjTNGzREIpl"
      },
      "source": [
        "Where the `LightningModule` organizes our model and its optimizers,\n",
        "the `LightningDataModule` organizes our dataloading code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_KkQ0iOWKD7"
      },
      "source": [
        "The class-level docstring explains the concept\n",
        "behind the class well\n",
        "and lists the main methods to be over-ridden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "IFTWHdsFV5WG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A DataModule standardizes the training, val, test splits, data preparation and transforms. The main\n",
            "    advantage is consistent data splits, data preparation and transforms across models.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        class MyDataModule(LightningDataModule):\n",
            "            def __init__(self):\n",
            "                super().__init__()\n",
            "            def prepare_data(self):\n",
            "                # download, split, etc...\n",
            "                # only called on 1 GPU/TPU in distributed\n",
            "            def setup(self, stage):\n",
            "                # make assignments here (val/train/test split)\n",
            "                # called on every process in DDP\n",
            "            def train_dataloader(self):\n",
            "                train_split = Dataset(...)\n",
            "                return DataLoader(train_split)\n",
            "            def val_dataloader(self):\n",
            "                val_split = Dataset(...)\n",
            "                return DataLoader(val_split)\n",
            "            def test_dataloader(self):\n",
            "                test_split = Dataset(...)\n",
            "                return DataLoader(test_split)\n",
            "            def teardown(self):\n",
            "                # clean up after fit or test\n",
            "                # called on every process in DDP\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "print(pl.LightningDataModule.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLiacppGB9BB"
      },
      "source": [
        "Let's upgrade our `CorrelatedDataset` from a PyTorch `Dataset` to a `LightningDataModule`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "m1d62iC6Xv1i"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class CorrelatedDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, size=10_000, train_frac=0.8, batch_size=32):\n",
        "        super().__init__()  # again, mandatory superclass init, as with torch.nn.Modules\n",
        "\n",
        "        # set some constants, like the train/val split\n",
        "        self.size = size\n",
        "        self.train_frac, self.val_frac = train_frac, 1 - train_frac\n",
        "        self.train_indices = list(range(math.floor(self.size * train_frac)))\n",
        "        self.val_indices = list(range(self.train_indices[-1], self.size))\n",
        "\n",
        "        # under the hood, we've still got a torch Dataset\n",
        "        self.dataset = CorrelatedDataset(N=size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQf-jUYRCi3m"
      },
      "source": [
        "`LightningDataModule`s are designed to work in distributed settings,\n",
        "where operations that set state\n",
        "(e.g. writing to disk or attaching something to `self` that you want to access later)\n",
        "need to be handled with care.\n",
        "\n",
        "Getting data ready for training is often a very stateful operation,\n",
        "so the `LightningDataModule` provides two separate methods for it:\n",
        "one called `setup` that handles any state that needs to be set up in each copy of the module\n",
        "(here, splitting the data and adding it to `self`)\n",
        "and one called `prepare_data` that handles any state that only needs to be set up in each machine\n",
        "(for example, downloading data from storage and writing it to the local disk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mttu--rHX70r"
      },
      "outputs": [],
      "source": [
        "def setup(self, stage=None):  # prepares state that needs to be set for each GPU on each node\n",
        "    if stage == \"fit\" or stage is None:  # other stages: \"test\", \"predict\"\n",
        "        self.train_dataset = torch.utils.data.Subset(self.dataset, self.train_indices)\n",
        "        self.val_dataset = torch.utils.data.Subset(self.dataset, self.val_indices)\n",
        "\n",
        "def prepare_data(self):  # prepares state that needs to be set once per node\n",
        "    pass  # but we don't have any \"node-level\" computations\n",
        "\n",
        "\n",
        "CorrelatedDataModule.setup, CorrelatedDataModule.prepare_data = setup, prepare_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh3mZrjwD83Y"
      },
      "source": [
        "We then define methods to return `DataLoader`s when requested by the `Trainer`.\n",
        "\n",
        "To run a testing loop that uses a `LightningDataModule`,\n",
        "you'll also need to define a `test_dataloader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "xu9Ma3iKYPBd"
      },
      "outputs": [],
      "source": [
        "def train_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
        "    return torch.utils.data.DataLoader(self.train_dataset, batch_size=32, num_workers=8)\n",
        "\n",
        "def val_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
        "    return torch.utils.data.DataLoader(self.val_dataset, batch_size=32, num_workers=8)\n",
        "\n",
        "CorrelatedDataModule.train_dataloader, CorrelatedDataModule.val_dataloader = train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mInit signature:\u001b[0m\n",
            "\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mcollate_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mprefetch_factor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mpersistent_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSource:\u001b[0m        \n",
            "\u001b[0;32mclass\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGeneric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"\u001b[0m\n",
            "\u001b[0;34m    Data loader. Combines a dataset and a sampler, and provides an iterable over\u001b[0m\n",
            "\u001b[0;34m    the given dataset.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m    The :class:`~torch.utils.data.DataLoader` supports both map-style and\u001b[0m\n",
            "\u001b[0;34m    iterable-style datasets with single- or multi-process loading, customizing\u001b[0m\n",
            "\u001b[0;34m    loading order and optional automatic batching (collation) and memory pinning.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m    See :py:mod:`torch.utils.data` documentation page for more details.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m    Args:\u001b[0m\n",
            "\u001b[0;34m        dataset (Dataset): dataset from which to load the data.\u001b[0m\n",
            "\u001b[0;34m        batch_size (int, optional): how many samples per batch to load\u001b[0m\n",
            "\u001b[0;34m            (default: ``1``).\u001b[0m\n",
            "\u001b[0;34m        shuffle (bool, optional): set to ``True`` to have the data reshuffled\u001b[0m\n",
            "\u001b[0;34m            at every epoch (default: ``False``).\u001b[0m\n",
            "\u001b[0;34m        sampler (Sampler or Iterable, optional): defines the strategy to draw\u001b[0m\n",
            "\u001b[0;34m            samples from the dataset. Can be any ``Iterable`` with ``__len__``\u001b[0m\n",
            "\u001b[0;34m            implemented. If specified, :attr:`shuffle` must not be specified.\u001b[0m\n",
            "\u001b[0;34m        batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\u001b[0m\n",
            "\u001b[0;34m            returns a batch of indices at a time. Mutually exclusive with\u001b[0m\n",
            "\u001b[0;34m            :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\u001b[0m\n",
            "\u001b[0;34m            and :attr:`drop_last`.\u001b[0m\n",
            "\u001b[0;34m        num_workers (int, optional): how many subprocesses to use for data\u001b[0m\n",
            "\u001b[0;34m            loading. ``0`` means that the data will be loaded in the main process.\u001b[0m\n",
            "\u001b[0;34m            (default: ``0``)\u001b[0m\n",
            "\u001b[0;34m        collate_fn (Callable, optional): merges a list of samples to form a\u001b[0m\n",
            "\u001b[0;34m            mini-batch of Tensor(s).  Used when using batched loading from a\u001b[0m\n",
            "\u001b[0;34m            map-style dataset.\u001b[0m\n",
            "\u001b[0;34m        pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\u001b[0m\n",
            "\u001b[0;34m            into device/CUDA pinned memory before returning them.  If your data elements\u001b[0m\n",
            "\u001b[0;34m            are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\u001b[0m\n",
            "\u001b[0;34m            see the example below.\u001b[0m\n",
            "\u001b[0;34m        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\u001b[0m\n",
            "\u001b[0;34m            if the dataset size is not divisible by the batch size. If ``False`` and\u001b[0m\n",
            "\u001b[0;34m            the size of dataset is not divisible by the batch size, then the last batch\u001b[0m\n",
            "\u001b[0;34m            will be smaller. (default: ``False``)\u001b[0m\n",
            "\u001b[0;34m        timeout (numeric, optional): if positive, the timeout value for collecting a batch\u001b[0m\n",
            "\u001b[0;34m            from workers. Should always be non-negative. (default: ``0``)\u001b[0m\n",
            "\u001b[0;34m        worker_init_fn (Callable, optional): If not ``None``, this will be called on each\u001b[0m\n",
            "\u001b[0;34m            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\u001b[0m\n",
            "\u001b[0;34m            input, after seeding and before data loading. (default: ``None``)\u001b[0m\n",
            "\u001b[0;34m        multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If\u001b[0m\n",
            "\u001b[0;34m            ``None``, the default `multiprocessing context`_ of your operating system will\u001b[0m\n",
            "\u001b[0;34m            be used. (default: ``None``)\u001b[0m\n",
            "\u001b[0;34m        generator (torch.Generator, optional): If not ``None``, this RNG will be used\u001b[0m\n",
            "\u001b[0;34m            by RandomSampler to generate random indexes and multiprocessing to generate\u001b[0m\n",
            "\u001b[0;34m            ``base_seed`` for workers. (default: ``None``)\u001b[0m\n",
            "\u001b[0;34m        prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\u001b[0m\n",
            "\u001b[0;34m            in advance by each worker. ``2`` means there will be a total of\u001b[0m\n",
            "\u001b[0;34m            2 * num_workers batches prefetched across all workers. (default value depends\u001b[0m\n",
            "\u001b[0;34m            on the set value for num_workers. If value of num_workers=0 default is ``None``.\u001b[0m\n",
            "\u001b[0;34m            Otherwise, if value of ``num_workers > 0`` default is ``2``).\u001b[0m\n",
            "\u001b[0;34m        persistent_workers (bool, optional): If ``True``, the data loader will not shut down\u001b[0m\n",
            "\u001b[0;34m            the worker processes after a dataset has been consumed once. This allows to\u001b[0m\n",
            "\u001b[0;34m            maintain the workers `Dataset` instances alive. (default: ``False``)\u001b[0m\n",
            "\u001b[0;34m        pin_memory_device (str, optional): the device to :attr:`pin_memory` to if ``pin_memory`` is\u001b[0m\n",
            "\u001b[0;34m            ``True``.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m    .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\u001b[0m\n",
            "\u001b[0;34m                 cannot be an unpicklable object, e.g., a lambda function. See\u001b[0m\n",
            "\u001b[0;34m                 :ref:`multiprocessing-best-practices` on more details related\u001b[0m\n",
            "\u001b[0;34m                 to multiprocessing in PyTorch.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m    .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\u001b[0m\n",
            "\u001b[0;34m                 When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\u001b[0m\n",
            "\u001b[0;34m                 it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\u001b[0m\n",
            "\u001b[0;34m                 rounding depending on :attr:`drop_last`, regardless of multi-process loading\u001b[0m\n",
            "\u001b[0;34m                 configurations. This represents the best guess PyTorch can make because PyTorch\u001b[0m\n",
            "\u001b[0;34m                 trusts user :attr:`dataset` code in correctly handling multi-process\u001b[0m\n",
            "\u001b[0;34m                 loading to avoid duplicate data.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m                 However, if sharding results in multiple workers having incomplete last batches,\u001b[0m\n",
            "\u001b[0;34m                 this estimate can still be inaccurate, because (1) an otherwise complete batch can\u001b[0m\n",
            "\u001b[0;34m                 be broken into multiple ones and (2) more than one batch worth of samples can be\u001b[0m\n",
            "\u001b[0;34m                 dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\u001b[0m\n",
            "\u001b[0;34m                 cases in general.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m                 See `Dataset Types`_ for more details on these two types of datasets and how\u001b[0m\n",
            "\u001b[0;34m                 :class:`~torch.utils.data.IterableDataset` interacts with\u001b[0m\n",
            "\u001b[0;34m                 `Multi-process data loading`_.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m    .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\u001b[0m\n",
            "\u001b[0;34m                 :ref:`data-loading-randomness` notes for random seed related questions.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m    .. _multiprocessing context:\u001b[0m\n",
            "\u001b[0;34m        https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\u001b[0m\n",
            "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0mprefetch_factor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0m_iterator\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_BaseDataLoaderIter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0m__initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                 \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSampler\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                 \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_collate_fn_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                 \u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_worker_init_fn_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                 \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefetch_factor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                 \u001b[0mpersistent_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                 \u001b[0mpin_memory_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"python.data_loader\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_workers option should be non-negative; '\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                             \u001b[0;34m'use num_workers=0 to disable multiprocessing.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeout option should be non-negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprefetch_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prefetch_factor option could only be specified in multiprocessing.'\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                             \u001b[0;34m'let num_workers > 0 to enable multiprocessing, otherwise set prefetch_factor to None.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprefetch_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mprefetch_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mprefetch_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprefetch_factor\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prefetch_factor option should be non-negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mpersistent_workers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'persistent_workers option needs num_workers > 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefetch_factor\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_device\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_init_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiprocessing_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# Adds forward compatibilities so classic DataLoader can work with DataPipes:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#   _DataPipeSerializationWrapper container makes it easier to serialize without redefining pickler\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterDataPipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IterDataPipeSerializationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapDataPipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MapDataPipeSerializationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# Arg-check dataset related before checking samplers because we want to\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# tell users that iterable-style datasets are incompatible with custom\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# samplers first, so that they don't learn that this combo doesn't work\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# after spending time fixing the custom sampler errors.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# NOTE [ Custom Samplers and IterableDataset ]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# `IterableDataset` does not support custom `batch_sampler` or\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# `sampler` since the key is irrelevant (unless we support\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# generator-style dataset one day...).\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# For `sampler`, we always create a dummy sampler. This is an\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# infinite sampler even when the dataset may have an implemented\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# finite `__len__` because in multi-process data loading, naive\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# settings will return duplicated data (which may be desired), and\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# thus using a sampler with length matching that of dataset will\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# cause data lost (you may have duplicates of the first couple\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# batches, but never see anything afterwards). Therefore,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# `Iterabledataset` always uses an infinite sampler, an instance of\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# `_InfiniteConstantSampler` defined above.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# A custom `batch_sampler` essentially only controls the batch size.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# However, it is unclear how useful it would be since an iterable-style\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# dataset can handle that within itself. Moreover, it is pointless\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# in multi-process data loading as the assignment order of batches\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# to workers is an implementation detail so users can not control\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# how to batchify each worker's iterable. Thus, we disable this\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# option. If this turns out to be useful in future, we can re-enable\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# this, and support custom samplers that specify the assignments to\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# specific workers.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterDataPipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_shuffle_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# We cannot check `shuffle is not None` here, since previously `shuffle=False` was the default.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0;34mf\"DataLoader with IterableDataset: expected unspecified shuffle option, but got shuffle={shuffle}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;31m# See NOTE [ Custom Samplers and IterableDataset ]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0;34mf\"DataLoader with IterableDataset: expected unspecified sampler option, but got sampler={sampler}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mbatch_sampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;31m# See NOTE [ Custom Samplers and IterableDataset ]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"DataLoader with IterableDataset: expected unspecified \"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0;34mf\"batch_sampler option, but got batch_sampler={batch_sampler}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMap\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sampler option is mutually exclusive with '\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                             \u001b[0;34m'shuffle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mbatch_sampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# auto_collation with custom batch_sampler\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_sampler option is mutually exclusive '\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                                 \u001b[0;34m'with batch_size, shuffle, sampler, and '\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                                 \u001b[0;34m'drop_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mdrop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# no auto_collation\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_size=None option disables auto-batching '\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                                 \u001b[0;34m'and is mutually exclusive with drop_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# give default samplers\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;31m# See NOTE [ Custom Samplers and IterableDataset ]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_InfiniteConstantSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# auto_collation without custom batch_sampler\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mbatch_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_convert\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_workers\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IterableDataset_len_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# See NOTE [ IterableDataset and __len__ ]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_vital\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataloader'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'enabled'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'_BaseDataLoaderIter'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__multiprocessing_context\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmultiprocessing_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0mvalid_start_methods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_start_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mmultiprocessing_context\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_start_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                            \u001b[0;34m'multiprocessing_context option '\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                            \u001b[0;34mf'should specify a valid start method in {valid_start_methods!r}, but got '\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                            \u001b[0;34mf'multiprocessing_context={multiprocessing_context!r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0mmultiprocessing_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpython_multiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multiprocessing_context option should be a valid context '\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                                    \u001b[0;34m'object or a string specifying the start method, but got '\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                                    \u001b[0;34mf'multiprocessing_context={multiprocessing_context}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multiprocessing_context can only be used with '\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                                 \u001b[0;34m'multi-process loading (num_workers > 0), but got '\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                                 \u001b[0;34mf'num_workers={self.num_workers}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__multiprocessing_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__initialized\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_sampler'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sampler'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'drop_last'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'persistent_workers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{attr} attribute should not be set after {self.__class__.__name__} is initialized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;31m# We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;31m# since '_BaseDataLoaderIter' references 'DataLoader'.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'_BaseDataLoaderIter'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# When using a single worker the returned iterator should be\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# created everytime to avoid resetting its state\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# However, in the case of a multiple workers iterator\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# the iterator is only created once in the lifetime of the\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# DataLoader object so that workers can be reused\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_workers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_auto_collation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_index_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# The actual sampler used for generating indices for `_DatasetFetcher`\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# (see _utils/fetch.py) to read data at each time. This would be\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# `.batch_sampler` if in auto-collation mode, and `.sampler` otherwise.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# We can't change `.sampler` and `.batch_sampler` attributes for BC\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# reasons.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# NOTE [ IterableDataset and __len__ ]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# For `IterableDataset`, `__len__` could be inaccurate when one naively\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# does multi-processing data loading, since the samples will be duplicated.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# However, no real use case should be actually using that behavior, so\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# it should count as a user error. We should generally trust user\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# code to do the proper thing (e.g., configure each replica differently\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# in `__iter__`), and give us the correct `__len__` if they choose to\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# implement it (this will still throw if the dataset does not implement\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# a `__len__`).\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# To provide a further warning, we track if `__len__` was called on the\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# `DataLoader`, save the returned value in `self._len_called`, and warn\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# if the iterator ends up yielding more than this number of samples.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# Cannot statically verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IterableDataset_len_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment, arg-type]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# IterableDataset doesn't allow custom sampler or batch_sampler\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mceil\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# This function check whether the dataloader's worker number is rational based on\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# current system's resource. Current rule is that if the number of workers this\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# Dataloader will create is bigger than the number of logical cpus that is allowed to\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# use, than we will pop up a warning to let user pay attention.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# eg. If current system has 2 physical CPUs with 16 cores each. And each core support 2\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#     threads, then the total logical cpus here is 2 * 16 * 2 = 64. Let's say current\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#     DataLoader process can use half of them which is 32, then the rational max number of\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#     worker that initiated from this process is 32.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#     Now, let's say the created DataLoader has num_works = 40, which is bigger than 32.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#     So the warning message is triggered to notify the user to lower the worker number if\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#     necessary.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# [Note] Please note that this function repects `cpuset` only when os.sched_getaffinity is\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#        available (available in most of Linux system, but not OSX and Windows).\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#        When os.sched_getaffinity is not available, os.cpu_count() is called instead, but\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#        it doesn't repect cpuset.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#        We don't take threading into account since each worker process is single threaded\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#        at this time.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#        We don't set any threading flags (eg. OMP_NUM_THREADS, MKL_NUM_THREADS, etc)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#        other than `torch.set_num_threads` to 1 in the worker process, if the passing\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#        in functions use 3rd party modules that rely on those threading flags to determine\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#        how many thread to create (eg. numpy, etc), then it is caller's responsibility to\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m#        set those flags correctly.\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mdef\u001b[0m \u001b[0m_create_warning_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_worker_suggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_worker_created\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpuset_checked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0msuggested_max_worker_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Our suggested max number of worker in current system is {}{}, which is smaller \"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;34m\"than what this DataLoader is going to create.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0mnum_worker_suggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcpuset_checked\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\" (`cpuset` is not taken into account)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_worker_suggest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;34m\"DataLoader is not able to compute a suggested max number of worker in current system.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mwarn_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;34m\"This DataLoader will create {} worker processes in total. {} \"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Please be aware that excessive worker creation might get DataLoader running slow or even freeze, \"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;34m\"lower the worker number to avoid potential slowness/freeze if necessary.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0mnum_worker_created\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                    \u001b[0msuggested_max_worker_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mwarn_msg\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# try to compute a suggested max number of worker based on system's resource\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mmax_num_worker_suggest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mcpuset_checked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sched_getaffinity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mmax_num_worker_suggest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched_getaffinity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mcpuset_checked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmax_num_worker_suggest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# os.cpu_count() could return Optional[int]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;31m# get cpu count first and check None in order to satisfy mypy check\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mcpu_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mcpu_count\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mmax_num_worker_suggest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmax_num_worker_suggest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_create_warning_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mmax_num_worker_suggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mcpuset_checked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_num_worker_suggest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_create_warning_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mmax_num_worker_suggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m                \u001b[0mcpuset_checked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFile:\u001b[0m           ~/fsdl-text-recognizer-2022-labs/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\n",
            "\u001b[0;31mType:\u001b[0m           type\n",
            "\u001b[0;31mSubclasses:\u001b[0m     "
          ]
        }
      ],
      "source": [
        "torch.utils.data.DataLoader??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNodiN6oawX5"
      },
      "source": [
        "Now we're ready to run training using a datamodule:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JKBwoE-Rajqw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/home/ubuntu/fsdl-text-recognizer-2022-labs/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:131: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
            "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type   | Params\n",
            "---------------------------------\n",
            "0 | model | Linear | 2     \n",
            "---------------------------------\n",
            "2         Trainable params\n",
            "0         Non-trainable params\n",
            "2         Total params\n",
            "0.000     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss before training: 1.7290900945663452\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/fsdl-text-recognizer-2022-labs/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c17527cb8842484b8ace85098a33df46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss after training: 1.0625207424163818\n"
          ]
        }
      ],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "print(\"loss before training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "trainer.fit(model=model, datamodule=datamodule)\n",
        "\n",
        "print(\"loss after training:\", torch.mean(torch.square(model(dataset.xs) - dataset.ys)).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw6flh5Jf2ZP"
      },
      "source": [
        "Notice the warning: \"`Skipping val loop.`\"\n",
        "\n",
        "It's being raised because our minimal `LinearRegression` model\n",
        "doesn't have a `.validation_step` method.\n",
        "\n",
        "In the exercises, you're invited to add a validation step and resolve this warning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJnoFx47ZjBw"
      },
      "source": [
        "In the FSDL codebase,\n",
        "we define the basic functions of a `LightningDataModule`\n",
        "in the `BaseDataModule` and defer details to subclasses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PTPKvDDGXmOr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mInit signature:\u001b[0m \u001b[0mBaseDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSource:\u001b[0m        \n",
            "\u001b[0;32mclass\u001b[0m \u001b[0mBaseDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningDataModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Base for all of our LightningDataModules.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m    Learn more at about LDMs at https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html\u001b[0m\n",
            "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_workers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_NUM_WORKERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpus\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;31m# Make sure to set the variables below in subclasses\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dims\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dims\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_val\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mdata_dirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA_DIRNAME\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0madd_to_argparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;34m\"--batch_size\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Number of examples to operate on per forward step. Default is {BATCH_SIZE}.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0;34m\"--num_workers\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_NUM_WORKERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Number of additional processes to load data. Default is {DEFAULT_NUM_WORKERS}.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Return important settings of the dataset, which will be passed to instantiate models.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"input_dims\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_dims\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mapping\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Take the first steps to prepare data for use.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m        Use this method to do things that might write to disk or that need to be done only from a single GPU\u001b[0m\n",
            "\u001b[0;34m        in distributed settings (so don't set state `self.x = y`).\u001b[0m\n",
            "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Perform final setup to prepare data for consumption by DataLoader.\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m        Here is where we typically split into train, validation, and test. This is done once per GPU in a DDP setting.\u001b[0m\n",
            "\u001b[0;34m        Should assign `torch Dataset` objects to self.data_train, self.data_val, and optionally self.data_test.\u001b[0m\n",
            "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFile:\u001b[0m           ~/fsdl-text-recognizer-2022-labs/lab02/text_recognizer/data/base_data_module.py\n",
            "\u001b[0;31mType:\u001b[0m           type\n",
            "\u001b[0;31mSubclasses:\u001b[0m     MNIST, EMNIST, EMNISTLines"
          ]
        }
      ],
      "source": [
        "from text_recognizer.data import BaseDataModule\n",
        "\n",
        "\n",
        "BaseDataModule??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3mRlZecwaKB4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mInit signature:\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSource:\u001b[0m        \n",
            "\u001b[0;32mclass\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseDataModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"MNIST DataModule.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOWNLOADED_DATA_DIRNAME\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTStem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDIMS\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTPUT_DIMS\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAPPING\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Download train and test MNIST data from PyTorch canonical source.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mTorchMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mTorchMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Split into train, val, test, and set dims.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mmnist_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVAL_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFile:\u001b[0m           ~/fsdl-text-recognizer-2022-labs/lab02/text_recognizer/data/mnist.py\n",
            "\u001b[0;31mType:\u001b[0m           type\n",
            "\u001b[0;31mSubclasses:\u001b[0m     "
          ]
        }
      ],
      "source": [
        "from text_recognizer.data.mnist import MNIST\n",
        "\n",
        "\n",
        "MNIST??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQbMY08qD-hm"
      },
      "source": [
        "## `pl.Callback`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVe7TSNvHK4K"
      },
      "source": [
        "Lightning's `Callback` class is used to add \"nice-to-have\" features\n",
        "to training, validation, and testing\n",
        "that aren't strictly necessary for any model to run\n",
        "but are useful for many models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzU76wgFGw9N"
      },
      "source": [
        "A \"callback\" is a unit of code that's meant to be called later,\n",
        "based on some trigger.\n",
        "\n",
        "It's a very flexible system, which is why\n",
        "`Callback`s are used internally to implement lots of important Lightning features,\n",
        "including some we've already discussed, like `ModelCheckpoint` for saving during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-msDjbKdHTxU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['BackboneFinetuning',\n",
              " 'BaseFinetuning',\n",
              " 'Callback',\n",
              " 'DeviceStatsMonitor',\n",
              " 'EarlyStopping',\n",
              " 'GPUStatsMonitor',\n",
              " 'XLAStatsMonitor',\n",
              " 'GradientAccumulationScheduler',\n",
              " 'LambdaCallback',\n",
              " 'LearningRateMonitor',\n",
              " 'ModelCheckpoint',\n",
              " 'ModelPruning',\n",
              " 'ModelSummary',\n",
              " 'BasePredictionWriter',\n",
              " 'ProgressBar',\n",
              " 'ProgressBarBase',\n",
              " 'QuantizationAwareTraining',\n",
              " 'RichModelSummary',\n",
              " 'RichProgressBar',\n",
              " 'StochasticWeightAveraging',\n",
              " 'Timer',\n",
              " 'TQDMProgressBar']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pl.callbacks.__all__  # builtin Callbacks from Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6WRNXtHHkbM"
      },
      "source": [
        "The triggers, or \"hooks\", here, are specific points in the training, validation, and testing loop.\n",
        "\n",
        "The names of the hooks generally explain when the hook will be called,\n",
        "but you can always check the documentation for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3iHjjnU8Hvgg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hooks:\n",
            "\ton_after_backward, on_batch_end, on_batch_start,\n",
            "\ton_before_accelerator_backend_setup, on_before_backward,\n",
            "\ton_before_optimizer_step, on_before_zero_grad, on_configure_sharded_model,\n",
            "\ton_epoch_end, on_epoch_start, on_exception, on_fit_end, on_fit_start,\n",
            "\ton_init_end, on_init_start, on_keyboard_interrupt, on_load_checkpoint,\n",
            "\ton_predict_batch_end, on_predict_batch_start, on_predict_end,\n",
            "\ton_predict_epoch_end, on_predict_epoch_start, on_predict_start,\n",
            "\ton_pretrain_routine_end, on_pretrain_routine_start, on_sanity_check_end,\n",
            "\ton_sanity_check_start, on_save_checkpoint, on_test_batch_end,\n",
            "\ton_test_batch_start, on_test_end, on_test_epoch_end, on_test_epoch_start,\n",
            "\ton_test_start, on_train_batch_end, on_train_batch_start, on_train_end,\n",
            "\ton_train_epoch_end, on_train_epoch_start, on_train_start,\n",
            "\ton_validation_batch_end, on_validation_batch_start, on_validation_end,\n",
            "\ton_validation_epoch_end, on_validation_epoch_start, on_validation_start\n"
          ]
        }
      ],
      "source": [
        "hooks = \", \".join([method for method in dir(pl.Callback) if method.startswith(\"on_\")])\n",
        "print(\"hooks:\", *textwrap.wrap(hooks, width=80), sep=\"\\n\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E2M7O2cGdj7"
      },
      "source": [
        "You can define your own `Callback` by inheriting from `pl.Callback`\n",
        "and over-riding one of the \"hook\" methods --\n",
        "much the same way that you define your own `LightningModule`\n",
        "by writing your own `.training_step` and `.configure_optimizers`.\n",
        "\n",
        "Let's define a silly `Callback` just to demonstrate the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "UodFQKAGEJlk"
      },
      "outputs": [],
      "source": [
        "class HelloWorldCallback(pl.Callback):\n",
        "\n",
        "    def on_train_epoch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        print(\"👋 hello from the start of the training epoch!\")\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        print(\"👋 hello from the end of the validation epoch!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU7oIpyEGoaP"
      },
      "source": [
        "This callback will print a message whenever the training epoch starts\n",
        "and whenever the validation epoch ends.\n",
        "\n",
        "Different \"hooks\" have different information directly available.\n",
        "\n",
        "For example, you can directly access the batch information\n",
        "inside the `on_train_batch_start` and `on_train_batch_end` hooks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "U17Qo_i_GCya"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def on_train_batch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):\n",
        "        if random.random() > 0.995:\n",
        "            print(f\"👋 hello from inside the lucky batch, #{batch_idx}!\")\n",
        "\n",
        "\n",
        "HelloWorldCallback.on_train_batch_start = on_train_batch_start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVKQXZOwQNGJ"
      },
      "source": [
        "We provide the callbacks when initializing the `Trainer`,\n",
        "then they are invoked during model fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-XHXZ64-ETCz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "model = LinearRegression()\n",
        "\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "trainer = pl.Trainer(  # we instantiate and provide the callback here, but nothing happens yet\n",
        "    max_epochs=10, gpus=int(torch.cuda.is_available()), callbacks=[HelloWorldCallback()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UEHUUhVOQv6K"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type   | Params\n",
            "---------------------------------\n",
            "0 | model | Linear | 2     \n",
            "---------------------------------\n",
            "2         Trainable params\n",
            "0         Non-trainable params\n",
            "2         Total params\n",
            "0.000     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb146c165d6442928d15ffddae50f066",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👋 hello from the start of the training epoch!\n",
            "👋 hello from inside the lucky batch, #128!\n",
            "👋 hello from inside the lucky batch, #230!\n",
            "👋 hello from the start of the training epoch!\n",
            "👋 hello from inside the lucky batch, #195!\n",
            "👋 hello from inside the lucky batch, #211!\n",
            "👋 hello from the start of the training epoch!\n",
            "👋 hello from inside the lucky batch, #175!\n",
            "👋 hello from the start of the training epoch!\n",
            "👋 hello from inside the lucky batch, #7!\n",
            "👋 hello from inside the lucky batch, #19!\n",
            "👋 hello from the start of the training epoch!\n",
            "👋 hello from inside the lucky batch, #165!\n",
            "👋 hello from inside the lucky batch, #221!\n",
            "👋 hello from the start of the training epoch!\n",
            "👋 hello from inside the lucky batch, #15!\n",
            "👋 hello from inside the lucky batch, #74!\n",
            "👋 hello from inside the lucky batch, #236!\n",
            "👋 hello from the start of the training epoch!\n",
            "👋 hello from inside the lucky batch, #177!\n",
            "👋 hello from the start of the training epoch!\n",
            "👋 hello from inside the lucky batch, #54!\n",
            "👋 hello from inside the lucky batch, #83!\n",
            "👋 hello from inside the lucky batch, #177!\n",
            "👋 hello from inside the lucky batch, #229!\n",
            "👋 hello from the start of the training epoch!\n",
            "👋 hello from inside the lucky batch, #119!\n",
            "👋 hello from inside the lucky batch, #161!\n",
            "👋 hello from the start of the training epoch!\n",
            "👋 hello from inside the lucky batch, #51!\n",
            "👋 hello from inside the lucky batch, #97!\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP2Xj1woFGwG"
      },
      "source": [
        "You can read more about callbacks in the documentation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "COHk5BZvFJN_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://pytorch-lightning.readthedocs.io/en/1.6.3/extensions/callbacks.html'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "callback_docs_url = f\"https://pytorch-lightning.readthedocs.io/en/{version}/extensions/callbacks.html\"\n",
        "callback_docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2K9e44iEGCR"
      },
      "source": [
        "## `torchmetrics`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO-UIFKyJCqJ"
      },
      "source": [
        "DNNs are also finicky and break silently:\n",
        "rather than crashing, they just start doing the wrong thing.\n",
        "Without careful monitoring, that wrong thing can be invisible\n",
        "until long after it has done a lot of damage to you, your team, or your users.\n",
        "\n",
        "We want to calculate metrics so we can monitor what's happening during training and catch bugs --\n",
        "or even achieve [\"observability\"](https://thenewstack.io/observability-a-3-year-retrospective/),\n",
        "meaning we can also determine\n",
        "how to fix bugs in training just by viewing logs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4YMyUI0Jr2f"
      },
      "source": [
        "But DNN training is also performance sensitive.\n",
        "Training runs for large language models have budgets that are\n",
        "more comparable to building an apartment complex\n",
        "than they are to the build jobs of traditional software pipelines.\n",
        "\n",
        "Slowing down training even a small amount can add a substantial dollar cost,\n",
        "obviating the benefits of catching and fixing bugs more quickly.\n",
        "\n",
        "Also implementing metric calculation during training adds extra work,\n",
        "much like the other software engineering best practices which it closely resembles,\n",
        "namely test-writing and monitoring.\n",
        "This distracts and detracts from higher-leverage research work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbvWjiHSIxzM"
      },
      "source": [
        "\n",
        "The `torchmetrics` library, which began its life as `pytorch_lightning.metrics`,\n",
        "resolves these issues by providing a `Metric` class that\n",
        "incorporates best performance practices,\n",
        "like smart accumulation across batches and over devices,\n",
        "defines a unified interface,\n",
        "and integrates with Lightning's built-in logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "21y3lgvwEKPC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "metrics:\n",
            "\tfunctional, Accuracy, AUC, AUROC, AveragePrecision, BinnedAveragePrecision,\n",
            "\tBinnedPrecisionRecallCurve, BinnedRecallAtFixedPrecision, BLEUScore,\n",
            "\tBootStrapper, CalibrationError, CatMetric, CHRFScore, CohenKappa,\n",
            "\tConfusionMatrix, CosineSimilarity, TweedieDevianceScore, ExplainedVariance,\n",
            "\tExtendedEditDistance, F1, F1Score, FBeta, FBetaScore, HammingDistance, Hinge,\n",
            "\tHingeLoss, JaccardIndex, KLDivergence, MatthewsCorrcoef, MatthewsCorrCoef,\n",
            "\tMaxMetric, MeanAbsoluteError, MeanAbsolutePercentageError, MeanMetric,\n",
            "\tMeanSquaredError, MeanSquaredLogError, Metric, MetricCollection, MetricTracker,\n",
            "\tMinMaxMetric, MinMetric, MultioutputWrapper,\n",
            "\tMultiScaleStructuralSimilarityIndexMeasure, PearsonCorrcoef, PearsonCorrCoef,\n",
            "\tPermutationInvariantTraining, PIT, Precision, PrecisionRecallCurve, PSNR,\n",
            "\tPeakSignalNoiseRatio, R2Score, Recall, RetrievalFallOut, RetrievalHitRate,\n",
            "\tRetrievalMAP, RetrievalMRR, RetrievalNormalizedDCG, RetrievalPrecision,\n",
            "\tRetrievalRecall, RetrievalRPrecision, ROC, SacreBLEUScore, SDR,\n",
            "\tSignalDistortionRatio, ScaleInvariantSignalDistortionRatio, SI_SDR, SI_SNR,\n",
            "\tScaleInvariantSignalNoiseRatio, SignalNoiseRatio, SNR, SpearmanCorrcoef,\n",
            "\tSpearmanCorrCoef, Specificity, SQuAD, SSIM, StructuralSimilarityIndexMeasure,\n",
            "\tStatScores, SumMetric, SymmetricMeanAbsolutePercentageError,\n",
            "\tTranslationEditRate, WER, WordErrorRate, CharErrorRate, MatchErrorRate,\n",
            "\tWordInfoLost, WordInfoPreserved\n"
          ]
        }
      ],
      "source": [
        "import torchmetrics\n",
        "\n",
        "\n",
        "tm_version = torchmetrics.__version__\n",
        "print(\"metrics:\", *textwrap.wrap(\", \".join(torchmetrics.__all__), width=80), sep=\"\\n\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TuPZkV1gfFE"
      },
      "source": [
        "Like the `LightningModule`, `torchmetrics.Metric` inherits from `torch.nn.Module`.\n",
        "\n",
        "That's because metric calculation, like module application, is typically\n",
        "1) an array-heavy computation that\n",
        "2) relies on persistent state\n",
        "(parameters for `Module`s, running values for `Metric`s) and\n",
        "3) benefits from acceleration and\n",
        "4) can be distributed over devices and nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "leiiI_QDS2_V"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "issubclass(torchmetrics.Metric, torch.nn.Module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy8MF2taP8MV"
      },
      "source": [
        "Documentation for the version of `torchmetrics` we're using can be found here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "LN4ashooP_tM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://torchmetrics.readthedocs.io/en/v0.7.3/'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torchmetrics_docs_url = f\"https://torchmetrics.readthedocs.io/en/v{tm_version}/\"\n",
        "torchmetrics_docs_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aycHhZNXwjr"
      },
      "source": [
        "In the `BaseLitModel`,\n",
        "we use the `torchmetrics.Accuracy` metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Vyq4IjmBXzTv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mSignature:\u001b[0m \u001b[0mBaseLitModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDocstring:\u001b[0m Initialize self.  See help(type(self)) for accurate signature.\n",
            "\u001b[0;31mSource:\u001b[0m   \n",
            "    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_config\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mapping\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_dims\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOSS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"transformer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_cycle_max_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"one_cycle_max_lr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_cycle_total_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"one_cycle_total_steps\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mONE_CYCLE_TOTAL_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
            "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFile:\u001b[0m      ~/fsdl-text-recognizer-2022-labs/lab02/text_recognizer/lit_models/base.py\n",
            "\u001b[0;31mType:\u001b[0m      function"
          ]
        }
      ],
      "source": [
        "BaseLitModel.__init__??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPoTH50YfkMF"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD_6PVAeflWw"
      },
      "source": [
        "### 🌟 Add a `validation_step` to the `LinearRegression` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "5KKbAN9eK281"
      },
      "outputs": [],
      "source": [
        "def validation_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "        xs, ys = batch  # unpack the batch\n",
        "        outs = self(xs)  # apply the model\n",
        "        loss = torch.nn.functional.mse_loss(outs, ys)  # compute the (squared error) loss\n",
        "        \n",
        "    return loss\n",
        "\n",
        "\n",
        "LinearRegression.validation_step = validation_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "AnPPHAPxFCEv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type   | Params\n",
            "---------------------------------\n",
            "0 | model | Linear | 2     \n",
            "---------------------------------\n",
            "2         Trainable params\n",
            "0         Non-trainable params\n",
            "2         Total params\n",
            "0.000     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c385c8156dc4b37b19bec0b6af98911",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3fb766aea9947f6ad6986603dbde152",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4203dfc7d5ad4a39b980a977b31d60a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df13178687a54507bfad56c08c477e72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b57b998a2cd34534a67afddb3f3a3bc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d04b4ad823db4faeaeda50bdcd3bba68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "563ddc480c304ea29159e28d659b54c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a5334b449ee4b85a787940ab14969cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "524cb6da4d7d4a73bdbfecb4b6c51435",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f20d1d682b274a82be5f820477468931",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddb52b4451154d6fa6bc8a720b62d7bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30736563596a41ada5bb18910eaba6b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "\n",
        "# if you code is working, you should see results for the validation loss in the output\n",
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u42zXktOFDhZ"
      },
      "source": [
        "### 🌟🌟 Add a `test_step` to the `LinearRegression` class and a `test_dataloader` to the `CorrelatedDataModule`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbWfqvumFESV"
      },
      "outputs": [],
      "source": [
        "def test_step(self: pl.LightningModule, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "    pass # your code here\n",
        "\n",
        "LinearRegression.test_step = test_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB96MpibLeJi"
      },
      "outputs": [],
      "source": [
        "class CorrelatedDataModuleWithTest(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, N=10_000, N_test=10_000):  # reimplement __init__ here\n",
        "        super().__init__()  # don't forget this!\n",
        "        self.dataset = None\n",
        "        self.test_dataset = None  # define a test set -- another sample from the same distribution\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        pass\n",
        "\n",
        "    def test_dataloader(self: pl.LightningDataModule) -> torch.utils.data.DataLoader:\n",
        "        pass  # create a dataloader for the test set here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jq3dcugMMOu"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModuleWithTest()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "\n",
        "# we run testing without fitting here\n",
        "trainer.test(model=model, datamodule=datamodule)  # if your code is working, you should see performance on the test set here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHg4MKmJPla6"
      },
      "source": [
        "### 🌟🌟🌟 Make a version of the `LinearRegression` class that calculates the `ExplainedVariance` metric during training and validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_1AKGWRR2ai"
      },
      "source": [
        "The \"variance explained\" is a useful metric for comparing regression models --\n",
        "its values are interpretable and comparable across datasets, unlike raw loss values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLecK4CsQWKk"
      },
      "source": [
        "Read the \"TorchMetrics in PyTorch Lightning\" guide for details on how to\n",
        "add metrics and metric logging\n",
        "to a `LightningModule`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWy0HyG4RYnX"
      },
      "outputs": [],
      "source": [
        "torchmetrics_guide_url = f\"https://torchmetrics.readthedocs.io/en/v{tm_version}/pages/lightning.html\"\n",
        "torchmetrics_guide_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoSQ3y6sSTvP"
      },
      "source": [
        "And check out the docs for `ExplainedVariance` to see how it's calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpGuRK2FRHh1"
      },
      "outputs": [],
      "source": [
        "print(torchmetrics.ExplainedVariance.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EAtpWXrSVR1"
      },
      "source": [
        "You'll want to start the `LinearRegression` class over from scratch,\n",
        "since the `__init__` and `{training, validation, test}_step` methods need to be rewritten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGtWt3_5SYTn"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFWNr1SfS5-r"
      },
      "source": [
        "You can test your code by running fitting and testing.\n",
        "\n",
        "To see whether it's working,\n",
        "[call `self.log` inside the `_step` methods](https://torchmetrics.readthedocs.io/en/v0.7.1/pages/lightning.html)\n",
        "with the\n",
        "[keyword argument `prog_bar=True`](https://pytorch-lightning.readthedocs.io/en/1.6.1/api/pytorch_lightning.core.LightningModule.html#pytorch_lightning.core.LightningModule.log).\n",
        "You should see the explained variance show up in the output alongside the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jse95DGCS6gR",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "datamodule = CorrelatedDataModule()\n",
        "\n",
        "dataset = datamodule.dataset\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=int(torch.cuda.is_available()))\n",
        "\n",
        "# if your code is working, you should see explained variance in the progress bar/logs\n",
        "trainer.fit(model=model, datamodule=datamodule)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab02a_lightning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f056848cf5d2396a4970b625f23716aa539c2ff5334414c1b5d98d7daae66f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
